{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will explore a dataset that describes websites with different features and labels them either benign or malicious . We will use supervised learning algorithms to figure out what feature patterns malicious websites are likely to have and use our model to predict malicious websites.\n",
    "\n",
    "# Challenge 1 - Explore The Dataset\n",
    "\n",
    "Let's start by exploring the dataset. First load the data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv('../website.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data from an bird's-eye view.\n",
    "\n",
    "You should already been very familiar with the procedures now so we won't provide the instructions step by step. Reflect on what you did in the previous labs and explore the dataset.\n",
    "\n",
    "Things you'll be looking for:\n",
    "\n",
    "* What the dataset looks like?\n",
    "* What are the data types?\n",
    "* Which columns contain the features of the websites?\n",
    "* Which column contains the feature we will predict? What is the code standing for benign vs malicious websites?\n",
    "* Do we need to transform any of the columns from categorical to ordinal values? If so what are these columns?\n",
    "\n",
    "Feel free to add additional cells for your explorations. Make sure to comment what you find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "display(websites.head(),websites.dtypes,websites.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comment here:\n",
    "\n",
    "El dataset contiene 1781 registros y 21 atributos de los cuales 7 son object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, evaluate if the columns in this dataset are strongly correlated.\n",
    "\n",
    "In the Mushroom supervised learning lab we did recently, we mentioned we are concerned if our dataset has strongly correlated columns because if it is the case we need to choose certain ML algorithms instead of others. We need to evaluate this for our dataset now.\n",
    "\n",
    "Luckily, most of the columns in this dataset are ordinal which makes things a lot easier for us. In the next cells below, evaluate the level of collinearity of the data.\n",
    "\n",
    "We provide some general directions for you to consult in order to complete this step:\n",
    "\n",
    "1. You will create a correlation matrix using the numeric columns in the dataset.\n",
    "\n",
    "1. Create a heatmap using `seaborn` to visualize which columns have high collinearity.\n",
    "\n",
    "1. Comment on which columns you might need to remove due to high collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "object_columns = ['URL','CHARSET','SERVER','WHOIS_COUNTRY','WHOIS_STATEPRO','WHOIS_REGDATE','WHOIS_UPDATED_DATE']\n",
    "websites_numeric1 = websites.drop(object_columns,axis=1)\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "a4_dims = (11.7, 8.27)\n",
    "fig, ax = pyplot.subplots(figsize=a4_dims)\n",
    "sns.set()\n",
    "ax = sns.heatmap(websites_numeric1.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "websites_numeric2 = websites_numeric1.drop('REMOTE_APP_PACKETS',axis=1)\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "a4_dims = (11.7, 8.27)\n",
    "fig, ax = pyplot.subplots(figsize=a4_dims)\n",
    "sns.set()\n",
    "ax = sns.heatmap(websites_numeric2.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_numeric3 = websites_numeric2.drop('TCP_CONVERSATION_EXCHANGE',axis=1)\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "a4_dims = (11.7, 8.27)\n",
    "fig, ax = pyplot.subplots(figsize=a4_dims)\n",
    "sns.set()\n",
    "ax = sns.heatmap(websites_numeric3.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comment here\n",
    "\n",
    "De la matrix de correlación se puede establecer que las columnas con mayor correlación y que por lo tanto se podrían eliminar por sospechar que pueden ser poco discriminantes son: \n",
    "\n",
    "REMOTE_APP_PACKETS\n",
    "\n",
    "SOURCE_APP_PACKETS\n",
    "\n",
    "TCP_CONVERSATION_EXCHANGE\n",
    "\n",
    "NOTA: tengo mis dudas de como se debe interpretar la matriz de correlación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Remove Column Collinearity.\n",
    "\n",
    "From the heatmap you created, you should have seen at least 3 columns that can be removed due to high collinearity. Remove these columns from the dataset.\n",
    "\n",
    "Note that you should remove as few columns as you can. You don't have to remove all the columns at once. But instead, try removing one column, then produce the heatmap again to determine if additional columns should be removed. As long as the dataset no longer contains columns that are correlated for over 90%, you can stop. Also, keep in mind when two columns have high collinearity, you only need to remove one of them but not both.\n",
    "\n",
    "In the cells below, remove as few columns as you can to eliminate the high collinearity in the dataset. Make sure to comment on your way so that the instructional team can learn about your thinking process which allows them to give feedback. At the end, print the heatmap again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites_numeric4 = websites_numeric3.drop('SOURCE_APP_PACKETS',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comment here\n",
    "\n",
    "El análisis lo he hecho en el apartado anterior. He ido eliminando las columnas donde hay un par de columnas correlacionadas pero sin saber exactamente si esto es realmente coherente al no saber exactamente a que se refiere cada atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print heatmap again\n",
    "a4_dims = (11.7, 8.27)\n",
    "fig, ax = pyplot.subplots(figsize=a4_dims)\n",
    "sns.set()\n",
    "ax = sns.heatmap(websites_numeric4.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Handle Missing Values\n",
    "\n",
    "The next step would be handling missing values. **We start by examining the number of missing values in each column, which you will do in the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember in the previous labs, we drop a column if the column contains a high proportion of missing values. After dropping those problematic columns, we drop the rows with missing values.\n",
    "\n",
    "#### In the cells below, handle the missing values from the dataset. Remember to comment the rationale of your decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites.drop('CONTENT_LENGTH',axis=1,inplace=True)\n",
    "websites.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comment here\n",
    "Se elimina la columna CONTENT_LENGTH ya que posee un alto porcentaje de nulos (812 de 1781). Elimino también el la fila con valor nulo en la columna DNS_QUERY_TIMES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, examine the number of missing values in each column. \n",
    "\n",
    "If all cleaned, proceed. Otherwise, go back and do more cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine missing values in each column\n",
    "websites.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Handle `WHOIS_*` Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several categorical columns we need to handle. These columns are:\n",
    "\n",
    "* `URL`\n",
    "* `CHARSET`\n",
    "* `SERVER`\n",
    "* `WHOIS_COUNTRY`\n",
    "* `WHOIS_STATEPRO`\n",
    "* `WHOIS_REGDATE`\n",
    "* `WHOIS_UPDATED_DATE`\n",
    "\n",
    "How to handle string columns is always case by case. Let's start by working on `WHOIS_COUNTRY`. Your steps are:\n",
    "\n",
    "1. List out the unique values of `WHOIS_COUNTRY`.\n",
    "1. Consolidate the country values with consistent country codes. For example, the following values refer to the same country and should use consistent country code:\n",
    "    * `CY` and `Cyprus`\n",
    "    * `US` and `us`\n",
    "    * `SE` and `se`\n",
    "    * `GB`, `United Kingdom`, and `[u'GB'; u'UK']`\n",
    "\n",
    "#### In the cells below, fix the country values as intructed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites['WHOIS_COUNTRY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES = {'Cyprus':'CY','us':'US','se':'SE','United Kingdom':'GB',\"[u'GB'; u'UK']\":'GB'}\n",
    "websites.replace({\"WHOIS_COUNTRY\": COUNTRIES},inplace=True)\n",
    "websites['WHOIS_COUNTRY'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have fixed the country values, can we convert this column to ordinal now?\n",
    "\n",
    "Not yet. If you reflect on the previous labs how we handle categorical columns, you probably remember we ended up dropping a lot of those columns because there are too many unique values. Too many unique values in a column is not desirable in machine learning because it makes prediction inaccurate. But there are workarounds under certain conditions. One of the fixable conditions is:\n",
    "\n",
    "#### If a limited number of values account for the majority of data, we can retain these top values and re-label all other rare values.\n",
    "\n",
    "The `WHOIS_COUNTRY` column happens to be this case. You can verify it by print a bar chart of the `value_counts` in the next cell to verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites['WHOIS_COUNTRY'].value_counts().plot(kind='bar',figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After verifying, now let's keep the top 10 values of the column and re-label other columns with `OTHER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites['WHOIS_COUNTRY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def country(x, countries=['US', 'CA', 'ES', 'UK', 'AU', 'PA', 'JP', 'IN', 'CN']):\n",
    "    if x in countries:\n",
    "        return x\n",
    "    else:\n",
    "        return 'OTHER'\n",
    "\n",
    "websites['WHOIS_COUNTRY'] = websites['WHOIS_COUNTRY'].apply(lambda x: country(x))\n",
    "websites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites['WHOIS_COUNTRY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since `WHOIS_COUNTRY` has been re-labelled, we don't need `WHOIS_STATEPRO` any more because the values of the states or provinces may not be relevant any more. We'll drop this column.\n",
    "\n",
    "In addition, we will also drop `WHOIS_REGDATE` and `WHOIS_UPDATED_DATE`. These are the registration and update dates of the website domains. Not of our concerns.\n",
    "\n",
    "#### In the next cell, drop `['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites.drop(['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Handle Remaining Categorical Data & Convert to Ordinal\n",
    "\n",
    "Now print the `dtypes` of the data again. Besides `WHOIS_COUNTRY` which we already fixed, there should be 3 categorical columns left: `URL`, `CHARSET`, and `SERVER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `URL` is easy. We'll simply drop it because it has too many unique values that there's no way for us to consolidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites.drop('URL',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the unique value counts of `CHARSET`. You see there are only a few unique values. So we can keep it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites['CHARSET'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SERVER` is a little more complicated. Print its unique values and think about how you can consolidate those values.\n",
    "\n",
    "#### Before you think of your own solution, don't read the instructions that come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites['SERVER'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Think Hard](../think-hard.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comment here\n",
    "Se trata de una categoría que tiene muchos valores únicos pero que se pueden clusterizar creando pocas variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are so many unique values in the `SERVER` column, there are actually only 3 main server types: `Microsoft`, `Apache`, and `nginx`. Just check if each `SERVER` value contains any of those server types and re-label them. For `SERVER` values that don't contain any of those substrings, label with `Other`.\n",
    "\n",
    "At the end, your `SERVER` column should only contain 4 unique values: `Microsoft`, `Apache`, `nginx`, and `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def check_str(string):\n",
    "    if 'Microsoft' in string:\n",
    "        string = 'Microsoft'\n",
    "        return string\n",
    "    elif 'Apache' in string:\n",
    "        string = 'Apache'\n",
    "        return string\n",
    "    elif 'nginx' in string:\n",
    "        string = 'nginx'\n",
    "        return string\n",
    "    else:\n",
    "        string = 'Other'\n",
    "        return string\n",
    "    \n",
    "websites['SERVER'] = websites['SERVER'].apply(check_str)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Count `SERVER` value counts here\n",
    "websites['SERVER'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, all our categorical data are fixed now. **Let's convert them to ordinal data using Pandas' `get_dummies` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)).** Make sure you drop the categorical columns by passing `drop_first=True` to `get_dummies` as we don't need them any more. **Also, assign the data with dummie values to a new variable `website_dummy`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "website_dummy = pd.get_dummies(websites, columns=['WHOIS_COUNTRY', 'CHARSET', 'SERVER'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, inspect `website_dummy` to make sure the data and types are intended - there shouldn't be any categorical columns at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "display(website_dummy.dtypes)\n",
    "website_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Modeling, Prediction, and Evaluation\n",
    "\n",
    "We'll start off this section by splitting the data to train and test. **Name your 4 variables `X_train`, `X_test`, `y_train`, and `y_test`. Select 80% of the data for training and 20% for testing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_dummy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this lab, we will opt to use SVM. \n",
    "\n",
    "Support Vector Machines, or SVM, is an algorithm that aims to draw a line or a plane between the two groups such that they are linearly separable and the distance from the observations of each group to the line or plane is maximized. The goal of the algorithm is to find the line or plane that separates the groups. You can read more about this algorithm [here](https://en.wikipedia.org/wiki/Support_vector_machine).\n",
    "\n",
    "In the next cell, `svm` will be imported for you. **You will initialize the proper estimator, fit the training data, and predict the test data.**\n",
    "\n",
    "The `sklearn.svm` class documentation can be found [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm). By reading the documentation and searching online, the question you'll need to answer is **which SVM estimator to use**? When you choose the estimator, keep the following in mind:\n",
    "\n",
    "* Our data are categorical, not continuous.\n",
    "\n",
    "* We have removed the correlated columns. All columns we have right now are independent.\n",
    "\n",
    "If your statistical knowledge is not adequate at this moment, don't worry. Just play around and make an informed guess. We'll evaluate your prediction in the next step. If the prediction is unsatisfactory you can move back to this step to modify your estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we'll show you how to compute the accuracy of your prediction. The output score will show you how often your classifier is correct. If you have used the proper estimator, your accuracy score should be over 0.9. However, if your accuracy score is unsatisfactory, go back to the previous step to try another estimator until you produce a satisfactory accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos librerías\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos datasets para análisis de los modelos\n",
    "cols = [col for col in website_dummy.columns.values if col != \"Type\"]\n",
    "X = website_dummy[cols]\n",
    "y = website_dummy[\"Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función de regresión logistica con score (crossvalidation)\n",
    "def LRs(X,y,ns):\n",
    "    cls = LogisticRegression(solver='liblinear')\n",
    "    #cls.fit(X,y)\n",
    "    scores = cross_val_score(cls,X,y,cv=ns)\n",
    "    #print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LRs(X,y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función de SVM con score (crossvalidation)\n",
    "def SVMs(X,y,k,ns):\n",
    "    cls = svm.SVC(kernel=k,probability=True)\n",
    "    #cls.fit(X,y)\n",
    "    scores = cross_val_score(cls,X,y,cv=ns)\n",
    "    #print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVMs(X,y,'linear',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = svm.SVC(kernel='linear',probability=True)\n",
    "scores = cross_val_score(cls,X,y,cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función de RandomForest con score (crossvalidation)\n",
    "def RFs(X,y,n,ns):\n",
    "    cls = RandomForestClassifier(n_estimators=n)\n",
    "    #cls.fit(X,y)\n",
    "    scores = cross_val_score(cls,X,y,cv=ns)\n",
    "    #print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFs(X,y,20,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_k = ['linear','poly','rbf','sigmoid','precomputed']\n",
    "rf_e = [5,10,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_classifier(X,y,svm_k,rf_e,ns=5):\n",
    "    classifiers = []\n",
    "    classifiers.append(LRs(X,y,ns))\n",
    "    for i in range(len(svm_k)):\n",
    "        classifiers.append(SVMs(X,y,svm_k[i],ns))\n",
    "    for e in range(len(rf_e)):\n",
    "        classifiers.append(RFs(X,y,rf_e[i],ns))\n",
    "    best = max(classifiers)\n",
    "    if classifiers.index(best) == 0:\n",
    "        name = 'Logistic Regression'\n",
    "    elif 0 < classifiers.index(best) < 6:\n",
    "        name = 'Support Vector Machines'\n",
    "    elif classifiers.index(best) > 6:\n",
    "        name = 'Random Forest'\n",
    "    return print(\"Best classifier is a {} model with an accuracy of {}\".format(name,best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función de regresión logistica\n",
    "def LR(X,y):\n",
    "    cls = LogisticRegression(solver='liblinear')\n",
    "    cls.fit(X,y)\n",
    "    return print(cls.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer prediction accuracy\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge - Feature Scaling\n",
    "\n",
    "Problem-solving in machine learning is iterative. You can improve your model prediction with various techniques (there is a sweetspot for the time you spend and the improvement you receive though). Now you've completed only one iteration of ML analysis. There are more iterations you can conduct to make improvements. In order to be able to do that, you will need deeper knowledge in statistics and master more data analysis techniques. In this bootcamp, we don't have time to achieve that advanced goal. But you will make constant efforts after the bootcamp to eventually get there.\n",
    "\n",
    "However, now we do want you to learn one of the advanced techniques which is called *feature scaling*. The idea of feature scaling is to standardize/normalize the range of independent variables or features of the data. This can make the outliers more apparent so that you can remove them. This step needs to happen during Challenge 6 after you split the training and test data because you don't want to split the data again which makes it impossible to compare your results with and without feature scaling. For general concepts about feature scaling, click [here](https://en.wikipedia.org/wiki/Feature_scaling). To read deeper, click [here](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e).\n",
    "\n",
    "In the next cell, attempt to improve your model prediction accuracy by means of feature scaling. A library you can utilize is `sklearn.preprocessing.RobustScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)). You'll use the `RobustScaler` to fit and transform your `X_train`, then transform `X_test`. You will use SVM to fit and predict your transformed data and obtain the accuracy score in the same way. Compare the accuracy score with your normalized data with the previous accuracy data. Is there an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
